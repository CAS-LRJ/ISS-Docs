---
title: Sensors
layout: home
nav_order: 2
has_children: true
---

Sensor classes based on the data source, such as simulators like CARLA and BeamNG and real vehicles, play a pivotal role in the development and testing of autonomous driving systems. These sensors serve as the eyes and ears of self-driving vehicles, providing crucial data for navigation, perception, and decision-making algorithms. In this context, sensor simulation becomes a critical component of the development process, allowing engineers to assess the system's performance in a controlled environment before deploying it in the real world.

Simulators like CARLA and BeamNG are powerful tools for sensor simulation, offering a range of sensor classes that mimic the behavior of real-world sensors. These simulators generate synthetic data, replicating the sensory input that an autonomous vehicle would receive while driving. The sensors commonly simulated in these environments include:

1. **LiDAR (Light Detection and Ranging)**: Simulated LiDAR sensors emit laser beams and measure their reflection from surrounding objects, providing detailed 3D point clouds. These simulators replicate the physics of LiDAR, including beam divergence and range accuracy, allowing developers to evaluate their algorithms' performance in various scenarios.
![ISS Architecture](/assets/sensors/carla_lidar_point_cloud.jpg){: .center-image width="480"}

2. **Radar**: Radar sensors use radio waves to detect objects and their relative speed. Simulated radar sensors emulate the radar signal's propagation and reflection, including effects like interference and attenuation, to generate realistic sensor data.
![ISS Architecture](/assets/sensors/carla_sensors_radar.jpg){: .center-image width="480"}

3. **Camera**: Simulated cameras replicate the behavior of vision sensors, including factors such as lens distortion, exposure, and image noise. These simulations enable the testing of computer vision algorithms for tasks like object detection, lane tracking, and traffic sign recognition.
![ISS Architecture](/assets/sensors/carla_sensors_rgb.jpg){: .center-image width="480"}

4. **GPS and IMU (Inertial Measurement Unit)**: Simulated GPS and IMU sensors provide vehicle localization and orientation data. These sensors simulate satellite signals and vehicle movements, allowing developers to assess GPS-based localization and sensor fusion algorithms.

5. **Ultrasonic Sensors**: Simulated ultrasonic sensors model the way ultrasonic waves bounce off objects to measure distances. They are crucial for detecting nearby obstacles during parking and low-speed maneuvers.

6. **Wheel Odometry**: Simulated wheel odometry sensors estimate the vehicle's motion based on wheel rotations. These sensors aid in dead reckoning and can be used for localization when GPS signals are weak or unavailable.

Data generated by these simulated sensors is typically provided in formats that mimic real-world sensor outputs, such as point clouds for LiDAR, radar sweeps for radar, and image frames for cameras. Developers can configure the sensor properties and environmental conditions within the simulators to create diverse testing scenarios, including different weather conditions, traffic patterns, and road types.

In summary, sensor simulation in autonomous driving development, whether in simulators like CARLA and BeamNG or using real vehicles, is indispensable for verifying and fine-tuning perception and control algorithms. It allows developers to evaluate the robustness and reliability of these systems under various conditions, ultimately contributing to the safe deployment of autonomous vehicles on our roads.